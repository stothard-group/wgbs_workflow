from os.path import join
from os import listdir
import pandas as pd


# snakemake --snakefile workflow/snakefile --cluster "sbatch -N 1 -c {resources.cores} \
# --mem={resources.mem_mb} --time={resources.runtime} --account=account" --rerun-incomplete --printshellcmds -j 20

#########################################################################################
# Path to config file
#########################################################################################
configfile: 'config/WGBS.config.yaml'

#########################################################################################
# Set local variables
#########################################################################################
RAW_FASTQ_DIR = config["raw_fastq_dir"]
OUTPUT_DIR = "results_comparison/"
SPLIT_DIR = OUTPUT_DIR + "split/"
TRIMGALORE_DIR = OUTPUT_DIR + "trimmed/"
ALIGN_DIR = OUTPUT_DIR + "align/"
GENOME_DIR = config["genome_dir"]
GENOME_BASENAME = config["genome_base"]
RECAL_DIR = OUTPUT_DIR + "base_quality_recal/"
STAT_DIR = OUTPUT_DIR + "stat_and_coverage/"
CGMAP_DIR = OUTPUT_DIR + "cgmap/"
BENCHMARK_DIR = OUTPUT_DIR + "benchmark/"
SNPSINDELS_DIR = OUTPUT_DIR + "snps/"



READGROUP_FILE = config["readgroup_file"]
TRIMGALORE = config["trimgalore"]
CGMAPTOOLS = config["cgmaptools"]

MIN_COVERAGE_DEPTH = config["min_coverage_depth"]
n_of_chunks = config["n_of_chunks"]


#########################################################################################
# Define samples
#########################################################################################
samples, = glob_wildcards(join(RAW_FASTQ_DIR + "{sample}_R1.fastq.gz"))

def get_combined_samples():
    rg_list = pd.read_csv(READGROUP_FILE, sep='\t')
    sam_list_all = rg_list['SM'].tolist()
    sam_list = list(set(sam_list_all))
    return sam_list

list_of_samples = get_combined_samples()
#list_of_samples, = glob_wildcards(join(CGMAP_DIR + "{sam}.bayes_dynamicP.SNPs.vcf"))
#########################################################################################
# rule to trigger generation of target files
#########################################################################################
localrules: hard_filter

rule final:
    input:
   #     expand(SPLIT_DIR + "{sample}_R1_part{new}.fastq.gz", sample=samples, new=range(n_of_chunks)),
   #     expand(TRIMGALORE_DIR + "{sample}_R2_part{new}_val_2.fq.gz", sample=samples, new=range(n_of_chunks)), #  trimming
   #     GENOME_DIR + "Bisulfite_Genome/CT_conversion/BS_CT.1.bt2", #  build genome index
   #     expand(RECAL_DIR+ "{sample}.deduplicated.rgid.bam", sample=samples), # add read group IDs
   #     expand(RECAL_DIR + "{sample_list_val}.deduplicated.rgid.m.sorted.bam", sample_list_val=list_of_samples), #  sort merged bam files
   #     expand(RECAL_DIR + "{sample_list_val}.deduplicated.rgid.m.sorted.bam.bai", sample_list_val=list_of_samples), #  index sorted bismark files
   #     expand(STAT_DIR + "{sample_list_val}.alignment_stats.txt", sample_list_val=list_of_samples), #  calculate statistics
   #     expand(CGMAP_DIR + "{sample_list_val}.bayes_dynamicP.a.SNPs.vcf", sample_list_val=list_of_samples), #  get SNPs
   #     GENOME_DIR + GENOME_BASENAME + ".dict", #  create sequence dictionary for reference
        expand(CGMAP_DIR + "{sample_list_val}.bayes_dynamicP.a.SNPs.reheader.vcf.gz", sample_list_val=list_of_samples),
        expand(CGMAP_DIR + "{sample_list_val}.bayes_dynamicP.a.SNPs.reheader.vcf.gz.tbi", sample_list_val=list_of_samples),

        SNPSINDELS_DIR + "merged.bayes_dynamicP.a.SNPs.reheader.vcf.gz",
        SNPSINDELS_DIR+ "merged.bayes_dynamicP.a.SNPs.reheader.altonly.vcf",
        expand(SNPSINDELS_DIR + "merged.a.goodSNPs.vcf", sample_list_val=list_of_samples) #  get the good SNPs at the end


#########################################################################################
# split input files
#########################################################################################

rule split:
    input:
        r1 = RAW_FASTQ_DIR + "{sample}_R1.fastq.gz",
        r2 = RAW_FASTQ_DIR + "{sample}_R2.fastq.gz"

    threads: 1
    resources:
        cores = 1,
        runtime = 480,
        mem_mb = 20000,
    output:
        r1 = expand(SPLIT_DIR + "{{sample}}_R1_part{new}.fastq.gz", new=range(n_of_chunks)),
        r2 = expand(SPLIT_DIR + "{{sample}}_R2_part{new}.fastq.gz", new=range(n_of_chunks))
    params:
        r1 = lambda wildcards: SPLIT_DIR  + wildcards.sample + '_R1_part',
        r2 = lambda wildcards: SPLIT_DIR  + wildcards.sample + '_R2_part',
    shell:
        """
        module load java
        partition.sh -Xmx{resources.mem_mb}m in={input.r1} in2={input.r2} out={params.r1}%.fastq.gz \
                out2={params.r2}%.fastq.gz ways={n_of_chunks}; \
        touch {output.r1}; \
        touch {output.r2}
        """

#########################################################################################
# run trimgalore on raw data
#########################################################################################

# Use --2colour 20 for NovaSeq6000 samples
rule trimgalore:
    input:
        R1 = (SPLIT_DIR + "{sample}_R1_part{new}.fastq.gz"),
        R2 = (SPLIT_DIR + "{sample}_R2_part{new}.fastq.gz")
    output:
        R1 = TRIMGALORE_DIR + "{sample}_R1_part{new}_val_1.fq.gz",
        R2 = TRIMGALORE_DIR + "{sample}_R2_part{new}_val_2.fq.gz"
    benchmark:
        BENCHMARK_DIR + "trimgalore.{sample}_part{new}.txt"
    threads: 4
    resources:
        cores= 4,
        mem_mb= 8000,
        runtime= 360
    shell:
        """
        module load python/3.9
        module load fastqc
        {TRIMGALORE} --cores {threads}  --paired --gzip {input.R1} {input.R2}  -o {TRIMGALORE_DIR}
        """


#########################################################################################
# prep genome for bismark
#########################################################################################
rule prep_genome:
    input:
        GENOME_DIR + GENOME_BASENAME + ".fa"
    output:
        GENOME_DIR + "Bisulfite_Genome/CT_conversion/BS_CT.1.bt2"
    params:
        GENOME_DIR
    benchmark:
        BENCHMARK_DIR + "prep_genome.txt"
    threads: 4
    resources:
        cores= 4,
        mem_mb= 16000,
        runtime= 120
    shell:
        """
        module load samtools perl StdEnv/2020 bismark bowtie2
        bismark_genome_preparation --parallel {threads} {params} 
        """

#########################################################################################
# run bismark
#########################################################################################

rule align:
    input:
        R1 = TRIMGALORE_DIR + "{sample}_R1_part{new}_val_1.fq.gz",
        R2 = TRIMGALORE_DIR + "{sample}_R2_part{new}_val_2.fq.gz",
        g_complete = GENOME_DIR + "Bisulfite_Genome/CT_conversion/BS_CT.1.bt2"
    benchmark:
        BENCHMARK_DIR + "align.{sample}_part{new}.txt"
    params:
        tmpdir = "./tmp_{sample}_part{new}"
    threads: 8 # PARALLEL 4 ~ 24 THREADS AND 48GB RAM, DO NOT CHANGE WITHOUT READING BISMARK README
    resources:
        cores= 24,
        mem_mb= 64000,
        runtime= 900
    output:
        ALIGN_DIR + "{sample}_R1_part{new}_val_1_bismark_bt2_pe.bam"
    shell:
        """
        module load StdEnv/2020 samtools perl StdEnv/2020 bismark bowtie2/2.4.1
        bismark --genome {GENOME_DIR} -1 {input.R1} -2 {input.R2} \
        --old_flag --no_dovetail \
        --parallel {threads} \
        --temp_dir {params.tmpdir} \
        -o {ALIGN_DIR}
        """

#########################################################################################
# merge files and sort prior to deduplication
#########################################################################################

rule merge_bams:
    input:
        bam = expand(ALIGN_DIR + "{{sample}}_R1_part{new}_val_1_bismark_bt2_pe.bam",
            new=[str(i) for i in list(range(n_of_chunks))]),
    output:
        ALIGN_DIR + "{sample}.bismark.m.bam"
    resources:
        cores = 1,
        runtime = 700,
        mem_mb = 8000,
    params:
        bam = ' '.join(["I=" + ALIGN_DIR + "{sample}_R1_part" + str(new) + "_val_1_bismark_bt2_pe.bam" for new in \
                list(range(n_of_chunks))]),
        tmp = "tmp/"
    shell:
        """
        module load java
        module load picard
        export JAVA_TOOL_OPTIONS=-Xmx{resources.mem_mb}m
        java -Xmx{resources.mem_mb}m -jar $EBROOTPICARD/picard.jar MergeSamFiles {params.bam} O={output} \
        SORT_ORDER=queryname VALIDATION_STRINGENCY=SILENT TMP_DIR={params.tmp} MAX_RECORDS_IN_RAM=1000000
        """

#########################################################################################
# bisulfite data deduplication
#########################################################################################

rule dedup:
    input:
        ALIGN_DIR + "{sample}.bismark.m.bam"
    output:
        RECAL_DIR + "{sample}.deduplicated.bam"
    benchmark:
        BENCHMARK_DIR + "dedup.{sample}.txt"
    threads: 16
    resources:
        cores= 16,
        mem_mb= 64000,
        runtime= 640
    params:
        lambda wildcards: wildcards.sample
    shell:
        """
        module load StdEnv/2020 bowtie2  samtools perl bismark
        deduplicate_bismark -p \
        --output_dir {RECAL_DIR} -o {params} \
        {input}
        """

#########################################################################################
# add read group information # Note - do this here rather than with bt2
#########################################################################################


def get_rg_label(sample):
    split_sample = sample.split(".")
    if split_sample[3].startswith("NEBNext"):
        sample_id_part = split_sample[4]
    else:
        sample_id_part = split_sample[3]
    rg_list = pd.read_csv(READGROUP_FILE, sep='\t')
    line = rg_list[rg_list['SM'] == int(sample_id_part)].copy()
    ID = line['ID'].values[0]
    LB = line['LB'].values[0]
    SM = line['SM'].values[0]
    PL = line['PL'].values[0]
    PU = ID + "." + str(SM)
    rg_label = [ID, str(SM), PL, str(LB), PU]
    return rg_label


rule add_rgids:
    input:
        RECAL_DIR + "{sample}.deduplicated.bam"
    output:
        RECAL_DIR + "{sample}.deduplicated.rgid.bam"
    benchmark:
        BENCHMARK_DIR + "add_rgids.{sample}.txt"
    params:
        RG_label = lambda wildcards: get_rg_label('{sample}'.format(sample=wildcards.sample)),
        tmp = "tmp/"
    threads: 1
    resources:
        cores= 1,
        mem_mb= 2000,
        runtime= 420
    shell:
        """
        module load java
        module load picard
        java -Xmx{resources.mem_mb}m -jar $EBROOTPICARD/picard.jar \
        AddOrReplaceReadGroups \
        I={input} O={output} \
        RGID={params.RG_label[0]} \
        RGSM={params.RG_label[1]} \
        RGPL={params.RG_label[2]} \
        RGLB={params.RG_label[3]} \
        RGPU={params.RG_label[4]} \
        CREATE_INDEX=true VALIDATION_STRINGENCY=SILENT SORT_ORDER=queryname TMP_DIR={params.tmp}
        """

#########################################################################################
# merge sample files
#########################################################################################

def merge_sample_list(sample_name):
    """
    Generates an input line for Picard MergeSamFiles for samples that need to be aggregated
    :param sample_name: base name of a sample (not the whole file name)
    :return: -I FC1_L1_SAM1.bam -I FC1_L2_SAM1.bam -I FC2_L1_SAM1.bam
    """
    if os.path.exists(RECAL_DIR):
        aligndir_files = os.listdir(RECAL_DIR)
        m_bam_list = []
        for file in aligndir_files:
            if file.endswith(".deduplicated.rgid.bam"):
                m_bam_list.append(file)
        keep_bams = []
        for bam in m_bam_list:
            split_bam_name = bam.split(".")
            if sample_name == split_bam_name[4]:
                keep_bams.append(bam)
            elif sample_name == split_bam_name[3]:
                keep_bams.append(bam)
            else:
                pass
        input_string = ''
        for kept_bam in keep_bams:
            input_string = input_string + "I=" + RECAL_DIR + kept_bam + " "
        new_input_string = input_string[:-1]
    else:
        new_input_string=''
    return new_input_string

rule merge:
    input:
        expand(RECAL_DIR + "{sample}.deduplicated.rgid.bam", sample=samples)
    params:
        input_line = lambda wildcards: merge_sample_list('{val}'.format(val=wildcards.sample_list_val)),
        tmpdir= "./tmp_{sample_list_val}"
    benchmark:
        BENCHMARK_DIR + "merge.{sample_list_val}.txt"
    threads: 2
    resources:
        cores= 2,
        mem_mb= 2000,
        runtime= 420
    output:
        RECAL_DIR + "{sample_list_val}.deduplicated.rgid.m.bam"
    shell:
        """
        module load java
        module load picard
        mkdir -p {params.tmpdir}
        export JAVA_TOOL_OPTIONS=-Xmx{resources.mem_mb}m
        java -Xmx{resources.mem_mb}m -jar $EBROOTPICARD/picard.jar \
        MergeSamFiles \
        {params.input_line} \
        O={output} \
        SORT_ORDER=queryname TMP_DIR={params.tmpdir}
        """

#########################################################################################
# Sort merged bismark alignment files
#########################################################################################

rule sortsam:
    input:
        RECAL_DIR + "{sample_list_val}.deduplicated.rgid.m.bam"
    output:
        RECAL_DIR + "{sample_list_val}.deduplicated.rgid.m.sorted.bam"
    params:
        tmpdir = lambda wildcards: RECAL_DIR + "tmp_" + wildcards.sample_list_val
    benchmark:
        BENCHMARK_DIR + "sortsam.{sample_list_val}.txt"
    threads: 4
    resources:
        cores= 4,
        mem_mb= 64000,
        runtime= 500
    shell:
        """
        module load samtools
        samtools sort -o {output} -O bam -@ {threads} -m 4000m --write-index -T {params.tmpdir} {input}
        """


#########################################################################################
# Index sorted bismark files
#########################################################################################

rule b_index:
    input:
        RECAL_DIR + "{sample_list_val}.deduplicated.rgid.m.sorted.bam"
    output:
        RECAL_DIR + "{sample_list_val}.deduplicated.rgid.m.sorted.bam.bai"
    benchmark:
        BENCHMARK_DIR + "b_index.{sample_list_val}.txt"
    threads: 1
    resources:
        cores= 1,
        mem_mb= 500,
        runtime= 120
    shell:
        """
        module load samtools
        samtools index {input}
        """

#########################################################################################
# Get coverage and alignment statistics
#########################################################################################

rule coverage:
    input:
        align = RECAL_DIR + "{sample_list_val}.deduplicated.rgid.m.sorted.bam",
        index = RECAL_DIR + "{sample_list_val}.deduplicated.rgid.m.sorted.bam.bai"
    output:
        depth = STAT_DIR + "{sample_list_val}.average_coverage.txt",
        mpileup = STAT_DIR + "{sample_list_val}.bases_covered.txt",
        alignstats = STAT_DIR + "{sample_list_val}.alignment_stats.txt"
    benchmark:
        BENCHMARK_DIR + "coverage.{sample_list_val}.txt"
    threads: 1
    resources:
        cores= 1,
        mem_mb= 1000,
        runtime= 720
    shell:
        """
        module load samtools
        samtools depth {input.align} | awk '{{sum+=$3}} END {{ print "Average = ",sum/NR}}' > {output.depth}
        samtools mpileup {input.align} |  awk -v X="{MIN_COVERAGE_DEPTH}" '$4>=X' | wc -l > {output.mpileup}
        samtools flagstat {input.align} > {output.alignstats}
        """

#########################################################################################
# Make DNA methylation profiles (ATCGmaps) from alignments
#########################################################################################

rule cgmap_conversion:
    input:
        align = RECAL_DIR + "{sample_list_val}.deduplicated.rgid.m.sorted.bam",
        index = RECAL_DIR + "{sample_list_val}.deduplicated.rgid.m.sorted.bam.bai"
    output:
        CGMAP_DIR + "{sample_list_val}.ATCGmap.gz"
    benchmark:
        BENCHMARK_DIR + "cgmap_conversion.{sample_list_val}.txt"
    params:
        out = lambda wildcards: CGMAP_DIR + wildcards.sample_list_val,
        genome = GENOME_DIR + GENOME_BASENAME + ".fa"
    threads: 1
    resources:
        cores= 1,
        mem_mb= 2000,
        runtime= 3600
    shell:
        """
        {CGMAPTOOLS} convert bam2cgmap -b {input.align} \
        -g {params.genome} \
        --rmOverlap \
        -o {params.out}
        """

rule cgmap:
    input:
        CGMAP_DIR + "{sample_list_val}.ATCGmap.gz"
    output:
        vcf= CGMAP_DIR + "{sample_list_val}.bayes_dynamicP.a.SNPs.vcf",
        out = CGMAP_DIR + "{sample_list_val}.bayes_dynamicP.a.SNPs.snv"
    benchmark:
        BENCHMARK_DIR + "cgmap.{sample_list_val}.txt"
    threads: 1
    resources:
        cores= 1,
        mem_mb= 2000,
        runtime= 4320
    shell:
        """
        module load python/2.7
        {CGMAPTOOLS} snv -i {input} \
        -a -o {output.out} -v {output.vcf} \
        -m bayes --bayes-dynamicP
        """

rule compress:
    input:
        CGMAP_DIR + "{sample_list_val}.bayes_dynamicP.a.SNPs.vcf"
    output:
        gz = CGMAP_DIR + "{sample_list_val}.bayes_dynamicP.a.SNPs.vcf.gz",
    threads: 1
    resources:
        cores= 1,
        mem_mb= 2000,
        runtime= 360
    shell:
        """
        module load bcftools
        bgzip {input}
        tabix -p vcf {output.gz}
        """


rule sample_header:
    input:
        CGMAP_DIR + "{sample_list_val}.bayes_dynamicP.a.SNPs.vcf.gz"
    params:
        name = lambda wildcards: wildcards.sample_list_val,
        file = lambda wildcards: CGMAP_DIR + wildcards.sample_list_val + ".headerline.txt",
        old = lambda wildcards: CGMAP_DIR + wildcards.sample_list_val + ".oldheader.txt",
        new = lambda wildcards: CGMAP_DIR + wildcards.sample_list_val + ".newheader.txt",
    output:
        gz = CGMAP_DIR + "{sample_list_val}.bayes_dynamicP.a.SNPs.reheader.vcf.gz",
        tbi = CGMAP_DIR + "{sample_list_val}.bayes_dynamicP.a.SNPs.reheader.vcf.gz.tbi",
    threads: 1
    resources:
        cores= 1,
        mem_mb= 1000,
        runtime= 60
    shell:
        """
        module load bcftools
        echo {params.name} > {params.file}
        bcftools view -h {input} > {params.old}
        ./reheader.sh {params.old} {params.new}
        bcftools reheader -s {params.file} -h {params.new} -o {output.gz} {input}
        tabix -p vcf {output.gz}
        """

rule remove_vague:
    input:
        gz = CGMAP_DIR + "{sample_list_val}.bayes_dynamicP.a.SNPs.reheader.vcf.gz",
    output:
        vcf = CGMAP_DIR + "{sample_list_val}.bayes_dynamicP.a.SNPs.reheader.ATCG.vcf",
        gz = CGMAP_DIR + "{sample_list_val}.bayes_dynamicP.a.SNPs.reheader.ATCG.vcf.gz",
    params:
        val = lambda wildcards: wildcards.sample_list_val
    threads: 1
    resources:
        cores=1,
        mem_mb= 2000,
        runtime=720
    shell:
        """
        module load bcftools
        zcat {input} | grep -vP '^#'  > {params.val}.merged.vcf.body
        zcat {input} | grep -P '^#' > {output}
        cat {params.val}.merged.vcf.body | awk -v OFS='\t' '{{ if(($4!="N" && ($5=="A"||$5=="C"||$5=="G"||$5=="T"))) {{ print }} }}' >> {output.vcf}
        bgzip {output.vcf}
        tabix -p {output.gz}
        rm {params.val}.merged.vcf.body
        """


rule merge_bcftools:
    input:
        expand(CGMAP_DIR + "{sample_list_val}.bayes_dynamicP.a.SNPs.reheader.ATCG.vcf.gz", sample_list_val=list_of_samples)
    output:
        SNPSINDELS_DIR + "merged.bayes_dynamicP.a.SNPs.reheader.vcf.gz"
    threads: 1
    resources:
        cores= 1,
        mem_mb= 2000,
        runtime= 720
    shell:
        """
        module load bcftools
        bcftools merge -O z -m snps -o {output} {input}
        """

rule remove_ref:
    input:
        SNPSINDELS_DIR + "merged.bayes_dynamicP.a.SNPs.reheader.vcf.gz"
    output:
        SNPSINDELS_DIR + "merged.bayes_dynamicP.a.SNPs.reheader.altonly.vcf"
    threads: 1
    resources:
        cores= 1,
        mem_mb= 2000,
        runtime= 720
    shell:
        """
        module load bcftools
        bcftools view -i 'GT="alt"' -O v -o {output} {input}
        """


#########################################################################################
# Select bi-allelic SNPs
#########################################################################################
rule extract_snps:
    input:
        SNPSINDELS_DIR + "merged.bayes_dynamicP.a.SNPs.reheader.altonly.vcf"
    output:
        SNPSINDELS_DIR + "merged.a.rawSNPs.vcf"
    params:
        genome = GENOME_DIR + GENOME_BASENAME + ".fa"
    threads: 2
    resources:
        cores = 2,
        runtime = 60,
        mem_mb = 8000,
    shell:
        """
        module load gatk
        gatk SelectVariants \
            -R {params.genome} \
            -V {input} \
            --select-type-to-include SNP \
            --restrict-alleles-to BIALLELIC \
            -O {output}
        """

rule sequence_dict:
    input:
        GENOME_DIR + GENOME_BASENAME + ".fa"
    output:
        GENOME_DIR + GENOME_BASENAME + ".dict"
    threads: 1
    resources:
        cores= 1,
        mem_mb= 1000,
        runtime= 30
    shell:
        """
        module load java
        module load picard
        export JAVA_TOOL_OPTIONS=-Xmx{resources.mem_mb}m
        java -Xmx{resources.mem_mb}m -jar $EBROOTPICARD/picard.jar CreateSequenceDictionary \
        R={input} \
        O={output}
        """

rule filter:
    input:
        vcf = SNPSINDELS_DIR + "merged.a.rawSNPs.vcf",
        dict = GENOME_DIR + GENOME_BASENAME + ".dict",
    params:
        genome = GENOME_DIR + GENOME_BASENAME + ".fa",
        tmp = "tmp/"
    output:
        SNPSINDELS_DIR + "merged.a.filtSNPs.vcf"
    threads: 2
    resources:
        cores= 2,
        mem_mb= 8000,
        runtime= 360
    shell:
        """
        mkdir -p {params.tmp}
        module load gatk samtools
        export JAVA_TOOL_OPTIONS=-Xmx{resources.mem_mb}m
        gatk VariantFiltration \
        -R {params.genome} \
        -V {input.vcf} \
        --genotype-filter-expression 'DP < 10 || DP > 172' \
        --genotype-filter-name 'FAIL_DP' \
        -O {output} --tmp-dir {params.tmp}
        """

#########################################################################################
# Hard filter SNPs that pass quality check and are genotyped
#########################################################################################
rule hard_filter:
    input:
        SNPSINDELS_DIR + "merged.a.filtSNPs.vcf"
    output:
        SNPSINDELS_DIR + "merged.a.goodSNPs.vcf"
    shell:
        """
        module load bcftools
        bcftools view -O v -o {output}  -e 'GT ="." | (FMT/DP < 10 || FMT/DP > 172)' {input}
        """